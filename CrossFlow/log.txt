The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_cpu_threads_per_process` was set to `5` to improve out-of-box performance
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/opt/conda/envs/crossflow/lib/python3.8/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/opt/conda/envs/crossflow/lib/python3.8/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/opt/conda/envs/crossflow/lib/python3.8/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/opt/conda/envs/crossflow/lib/python3.8/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
I0414 11:44:14.084043 22581004211904 train_t2i.py:42] Process 0 using device: cuda:0
4ebf64dcfb814391b20452c5eacea59200001W:714713:714713 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
4ebf64dcfb814391b20452c5eacea59200001W:714713:714713 [0] NCCL INFO Bootstrap : Using eth0:10.0.0.7<0>
4ebf64dcfb814391b20452c5eacea59200001W:714713:714713 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
4ebf64dcfb814391b20452c5eacea59200001W:714713:714713 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
4ebf64dcfb814391b20452c5eacea59200001W:714713:714713 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.18.6+cuda12.1
I0414 11:44:14.569092 23139682244288 train_t2i.py:42] Process 2 using device: cuda:2
4ebf64dcfb814391b20452c5eacea59200001W:714715:714715 [2] NCCL INFO cudaDriverVersion 12020
4ebf64dcfb814391b20452c5eacea59200001W:714715:714715 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
4ebf64dcfb814391b20452c5eacea59200001W:714715:714715 [2] NCCL INFO Bootstrap : Using eth0:10.0.0.7<0>
4ebf64dcfb814391b20452c5eacea59200001W:714715:714715 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
4ebf64dcfb814391b20452c5eacea59200001W:714715:714715 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
I0414 11:44:14.784707 23426792027840 train_t2i.py:42] Process 1 using device: cuda:1
4ebf64dcfb814391b20452c5eacea59200001W:714714:714714 [1] NCCL INFO cudaDriverVersion 12020
4ebf64dcfb814391b20452c5eacea59200001W:714714:714714 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
4ebf64dcfb814391b20452c5eacea59200001W:714714:714714 [1] NCCL INFO Bootstrap : Using eth0:10.0.0.7<0>
4ebf64dcfb814391b20452c5eacea59200001W:714714:714714 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
4ebf64dcfb814391b20452c5eacea59200001W:714714:714714 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
I0414 11:44:14.817389 23181641606848 train_t2i.py:42] Process 3 using device: cuda:3
4ebf64dcfb814391b20452c5eacea59200001W:714716:714716 [3] NCCL INFO cudaDriverVersion 12020
4ebf64dcfb814391b20452c5eacea59200001W:714716:714716 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
4ebf64dcfb814391b20452c5eacea59200001W:714716:714716 [3] NCCL INFO Bootstrap : Using eth0:10.0.0.7<0>
4ebf64dcfb814391b20452c5eacea59200001W:714716:714716 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
4ebf64dcfb814391b20452c5eacea59200001W:714716:714716 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins/lib/libnccl-net.so
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO P2P plugin IBext
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB [RO]; OOB eth0:10.0.0.7<0>
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO Using network IBext
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins/lib/libnccl-net.so
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO P2P plugin IBext
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB [RO]; OOB eth0:10.0.0.7<0>
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Using network IBext
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins/lib/libnccl-net.so
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO P2P plugin IBext
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB [RO]; OOB eth0:10.0.0.7<0>
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO Using network IBext
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins/lib/libnccl-net.so
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO P2P plugin IBext
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB [RO]; OOB eth0:10.0.0.7<0>
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Using network IBext
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO comm 0xacd59c0 rank 3 nranks 4 cudaDev 3 nvmlDev 7 busId 800000 commId 0xe445dd4eed19e251 - Init START
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO comm 0x958ee90 rank 1 nranks 4 cudaDev 1 nvmlDev 5 busId 600000 commId 0xe445dd4eed19e251 - Init START
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO comm 0x9779350 rank 2 nranks 4 cudaDev 2 nvmlDev 6 busId 700000 commId 0xe445dd4eed19e251 - Init START
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO comm 0x98f19e0 rank 0 nranks 4 cudaDev 0 nvmlDev 4 busId 500000 commId 0xe445dd4eed19e251 - Init START
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Setting affinity for GPU 6 to ff,fff00000
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO NVLS multicast support is not available on dev 2
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Setting affinity for GPU 7 to ff,fff00000
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO NVLS multicast support is not available on dev 3
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO Setting affinity for GPU 4 to ff,fff00000
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO NVLS multicast support is not available on dev 0
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO Setting affinity for GPU 5 to ff,fff00000
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO NVLS multicast support is not available on dev 1
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO Channel 00/08 :    0   1   2   3
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] -1/-1/-1->1->2 [2] -1/-1/-1->1->2 [3] -1/-1/-1->1->3 [4] 2/-1/-1->1->0 [5] -1/-1/-1->1->2 [6] -1/-1/-1->1->2 [7] -1/-1/-1->1->3
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 2/-1/-1->3->0 [2] 2/-1/-1->3->0 [3] 1/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] 2/-1/-1->3->0 [6] 2/-1/-1->3->0 [7] 1/-1/-1->3->2
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO Channel 01/08 :    0   3   2   1
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 1/-1/-1->2->3 [2] 1/-1/-1->2->3 [3] 3/-1/-1->2->0 [4] 3/-1/-1->2->1 [5] 1/-1/-1->2->3 [6] 1/-1/-1->2->3 [7] 3/-1/-1->2->0
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO P2P Chunksize set to 524288
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO Channel 02/08 :    0   3   1   2
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO P2P Chunksize set to 524288
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO Channel 03/08 :    0   2   1   3
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO P2P Chunksize set to 524288
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO Channel 04/08 :    0   1   2   3
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO Channel 05/08 :    0   3   2   1
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO Channel 06/08 :    0   3   1   2
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO Channel 07/08 :    0   2   1   3
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 3/-1/-1->0->-1 [2] 3/-1/-1->0->-1 [3] 2/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 3/-1/-1->0->-1 [6] 3/-1/-1->0->-1 [7] 2/-1/-1->0->-1
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO P2P Chunksize set to 524288
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Channel 00/0 : 3[7] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO Channel 00/0 : 1[5] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Channel 03/0 : 3[7] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO Channel 02/0 : 1[5] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Channel 04/0 : 3[7] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO Channel 04/0 : 1[5] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Channel 00/0 : 2[6] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Channel 07/0 : 3[7] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO Channel 00/0 : 0[4] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO Channel 06/0 : 1[5] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Channel 04/0 : 2[6] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO Channel 04/0 : 0[4] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Channel 02/0 : 2[6] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Channel 02/0 : 3[7] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO Channel 03/0 : 1[5] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Channel 06/0 : 2[6] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO Channel 03/0 : 0[4] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Channel 06/0 : 3[7] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO Channel 07/0 : 1[5] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO Channel 07/0 : 0[4] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Channel 01/0 : 2[6] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO Channel 01/0 : 0[4] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Channel 03/0 : 2[6] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO Channel 02/0 : 0[4] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Channel 01/0 : 3[7] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO Channel 01/0 : 1[5] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Channel 05/0 : 2[6] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Channel 05/0 : 3[7] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO Channel 05/0 : 0[4] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Channel 07/0 : 2[6] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO Channel 05/0 : 1[5] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO Channel 06/0 : 0[4] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Connected all rings
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO Connected all rings
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO Channel 01/0 : 1[5] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Connected all rings
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO Connected all rings
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO Channel 05/0 : 1[5] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Channel 01/0 : 2[6] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Channel 02/0 : 2[6] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Channel 03/0 : 2[6] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Channel 05/0 : 2[6] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Channel 06/0 : 2[6] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Channel 01/0 : 3[7] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Channel 07/0 : 2[6] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Channel 02/0 : 3[7] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Channel 05/0 : 3[7] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Channel 06/0 : 3[7] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Channel 03/0 : 2[6] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Channel 03/0 : 3[7] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Channel 07/0 : 2[6] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Channel 07/0 : 3[7] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Channel 00/0 : 3[7] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Channel 02/0 : 3[7] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Channel 03/0 : 3[7] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Channel 04/0 : 3[7] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO Channel 00/0 : 1[5] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Channel 06/0 : 3[7] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO Channel 04/0 : 1[5] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Channel 07/0 : 3[7] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Channel 00/0 : 2[6] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Channel 02/0 : 2[6] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Channel 04/0 : 2[6] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Channel 06/0 : 2[6] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO Connected all trees
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO Connected all trees
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO Connected all trees
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO Connected all trees
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
4ebf64dcfb814391b20452c5eacea59200001W:714716:714785 [3] NCCL INFO comm 0xacd59c0 rank 3 nranks 4 cudaDev 3 nvmlDev 7 busId 800000 commId 0xe445dd4eed19e251 - Init COMPLETE
4ebf64dcfb814391b20452c5eacea59200001W:714714:714783 [1] NCCL INFO comm 0x958ee90 rank 1 nranks 4 cudaDev 1 nvmlDev 5 busId 600000 commId 0xe445dd4eed19e251 - Init COMPLETE
4ebf64dcfb814391b20452c5eacea59200001W:714713:714778 [0] NCCL INFO comm 0x98f19e0 rank 0 nranks 4 cudaDev 0 nvmlDev 4 busId 500000 commId 0xe445dd4eed19e251 - Init COMPLETE
4ebf64dcfb814391b20452c5eacea59200001W:714715:714780 [2] NCCL INFO comm 0x9779350 rank 2 nranks 4 cudaDev 2 nvmlDev 6 busId 700000 commId 0xe445dd4eed19e251 - Init COMPLETE
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
/opt/conda/envs/crossflow/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/opt/conda/envs/crossflow/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/opt/conda/envs/crossflow/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2025-04-14 11:44:17,689 - train_t2i.py - autoencoder:
  pretrained_path: /storage/v-jinpewang/lab_folder/qisheng_data/assets/stable-diffusion/autoencoder_kl.pth
  scale_factor: 0.2301
ckpt_root: /storage/v-jinpewang/lab_folder/qisheng_data/workdir/t2i_training_demo/default/ckpts
config_name: t2i_training_demo
dataset:
  cfg: false
  llm: t5
  name: textimage_features
  resolution: 256
  train_path: /storage/v-jinpewang/lab_folder/qisheng_data/raw_text_image_dataset_f10px512
  val_path: /storage/v-jinpewang/lab_folder/qisheng_data/raw_text_image_testset_f10px512
hparams: default
loss_coeffs: !!python/tuple
- 0.25
- 1
lr_scheduler:
  name: customized
  warmup_steps: 5000
mixed_precision: fp16
nnet:
  model_args: !!python/object:config_config.Args
    adapter_in_embed: 2048
    block_grad_to_lowres: false
    cfg_indicator: 0.1
    channels: 4
    clip_dim: 768
    gradient_checking: true
    norm_type: TDRMSN
    num_clip_token: 77
    stage_configs:
    - !!python/object:config_config.Args
      block_type: TransformerBlock
      dim: 1024
      dropout_prob: 0
      final_kernel_size: 3
      hidden_dim: 2048
      image_input_ratio: 1
      input_feature_ratio: 2
      max_height: 16
      max_width: 16
      num_attention_heads: 16
      num_blocks: 65
    - !!python/object:config_config.Args
      block_type: ConvNeXtBlock
      dim: 512
      dropout_prob: 0
      final_kernel_size: 3
      hidden_dim: 1024
      image_input_ratio: 1
      input_feature_ratio: 1
      kernel_size: 7
      max_height: 32
      max_width: 32
      num_blocks: 33
    textVAE: !!python/object:config_config.Args
      dropout_prob: 0.1
      hidden_dim: 1024
      hidden_token_length: 256
      num_attention_heads: 8
      num_blocks: 1
    use_t2i: true
  name: dimr
optimizer:
  betas: !!python/tuple
  - 0.9
  - 0.9
  lr: 1.0e-05
  name: adamw
  weight_decay: 0.03
pretrained_path: /storage/v-jinpewang/lab_folder/qisheng_data/t2i_256px_clip_dimr.pth
sample:
  cfg: false
  mini_batch_size: 5
  n_samples: 500
  path: /storage/v-jinpewang/lab_folder/qisheng_data/samplesave
  sample_steps: 50
  scale: 7
sample_dir: /storage/v-jinpewang/lab_folder/qisheng_data/workdir/t2i_training_demo/default/samples
seed: 1234
train:
  batch_size: 4
  eval_interval: 20000
  log_interval: 20000
  mode: cond
  n_samples_eval: 5
  n_steps: 800000
  save_interval: 20000
workdir: /storage/v-jinpewang/lab_folder/qisheng_data/workdir/t2i_training_demo/default
z_shape: !!python/tuple
- 4
- 32
- 32

2025-04-14 11:44:17,699 - train_t2i.py - Run on 4 devices
Prepare dataset...
Prepare dataset ok
/opt/conda/envs/crossflow/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
TransEncoder no compressor!
2025-04-14 11:44:31,022 - factory.py - Loaded ViT-L-16-SigLIP-256 model config.
成功加载基础参数，检测到 2 个adapter参数需要初始化
TransEncoder no compressor!
2025-04-14 11:44:54,809 - factory.py - Loaded ViT-L-16-SigLIP-256 model config.
params_decay 1688
params_nodecay 342
Create autoencoder with scale_factor=0.2301
making attention of type 'vanilla' with 512 in_channels
Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
making attention of type 'vanilla' with 512 in_channels
Using Janus-Pro-1B
2025-04-14 11:45:14,586 - train_t2i.py - Start fitting, step=0, mixed_precision=fp16
epoch:   0%|          | 0/2500 [00:00<?, ?it/s]通过VAE之前的shape: torch.Size([1, 77, 768])
通过VAE之后的shape: torch.Size([1, 8192])
/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/sigmoid/kernel.py:30: UserWarning: Using slower tdp_torch implementation for a tensor with shape torch.Size([1024])
  warnings.warn(f'Using slower tdp_torch implementation for a tensor with shape {param0.shape}')
/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/sigmoid/kernel.py:30: UserWarning: Using slower tdp_torch implementation for a tensor with shape torch.Size([67584])
  warnings.warn(f'Using slower tdp_torch implementation for a tensor with shape {param0.shape}')
/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/sigmoid/kernel.py:30: UserWarning: Using slower tdp_torch implementation for a tensor with shape torch.Size([512])
  warnings.warn(f'Using slower tdp_torch implementation for a tensor with shape {param0.shape}')
/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/sigmoid/kernel.py:30: UserWarning: Using slower tdp_torch implementation for a tensor with shape torch.Size([1024])
  warnings.warn(f'Using slower tdp_torch implementation for a tensor with shape {param0.shape}')
/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/sigmoid/kernel.py:30: UserWarning: Using slower tdp_torch implementation for a tensor with shape torch.Size([67584])
  warnings.warn(f'Using slower tdp_torch implementation for a tensor with shape {param0.shape}')
/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/sigmoid/kernel.py:30: UserWarning: Using slower tdp_torch implementation for a tensor with shape torch.Size([512])
  warnings.warn(f'Using slower tdp_torch implementation for a tensor with shape {param0.shape}')
/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/sigmoid/kernel.py:30: UserWarning: Using slower tdp_torch implementation for a tensor with shape torch.Size([1024])
  warnings.warn(f'Using slower tdp_torch implementation for a tensor with shape {param0.shape}')
/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/sigmoid/kernel.py:30: UserWarning: Using slower tdp_torch implementation for a tensor with shape torch.Size([1024])
  warnings.warn(f'Using slower tdp_torch implementation for a tensor with shape {param0.shape}')
/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/sigmoid/kernel.py:30: UserWarning: Using slower tdp_torch implementation for a tensor with shape torch.Size([67584])
  warnings.warn(f'Using slower tdp_torch implementation for a tensor with shape {param0.shape}')
/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/sigmoid/kernel.py:30: UserWarning: Using slower tdp_torch implementation for a tensor with shape torch.Size([512])
  warnings.warn(f'Using slower tdp_torch implementation for a tensor with shape {param0.shape}')
/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/sigmoid/kernel.py:30: UserWarning: Using slower tdp_torch implementation for a tensor with shape torch.Size([67584])
  warnings.warn(f'Using slower tdp_torch implementation for a tensor with shape {param0.shape}')
/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/sigmoid/kernel.py:30: UserWarning: Using slower tdp_torch implementation for a tensor with shape torch.Size([512])
  warnings.warn(f'Using slower tdp_torch implementation for a tensor with shape {param0.shape}')
/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [512, 1, 7, 7], strides() = [49, 1, 7, 1]
bucket_view.sizes() = [512, 1, 7, 7], strides() = [49, 49, 7, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [512, 1, 7, 7], strides() = [49, 1, 7, 1]
bucket_view.sizes() = [512, 1, 7, 7], strides() = [49, 49, 7, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [512, 1, 7, 7], strides() = [49, 1, 7, 1]
bucket_view.sizes() = [512, 1, 7, 7], strides() = [49, 49, 7, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [512, 1, 7, 7], strides() = [49, 1, 7, 1]
bucket_view.sizes() = [512, 1, 7, 7], strides() = [49, 49, 7, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
epoch:   0%|          | 1/2500 [00:51<35:54:24, 51.73s/it]通过VAE之前的shape: torch.Size([1, 77, 768])
通过VAE之后的shape: torch.Size([1, 8192])
epoch:   0%|          | 2/2500 [00:53<15:28:16, 22.30s/it]通过VAE之前的shape: torch.Size([1, 77, 768])
通过VAE之后的shape: torch.Size([1, 8192])
epoch:   0%|          | 3/2500 [00:54<8:43:36, 12.58s/it] 通过VAE之前的shape: torch.Size([1, 77, 768])
通过VAE之后的shape: torch.Size([1, 8192])
Traceback (most recent call last):
  File "train_t2i.py", line 362, in <module>
    app.run(main)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/absl/app.py", line 316, in run
    _run_main(main, args)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/absl/app.py", line 261, in _run_main
    sys.exit(main(argv))
  File "train_t2i.py", line 358, in main
    train(config)
  File "train_t2i.py", line 259, in train
    metrics = train_step(batch, ss_empty_context)
  File "train_t2i.py", line 174, in train_step
    loss, loss_dict = _flow_mathcing_model(_z, nnet, loss_coeffs=config.loss_coeffs, cond=_batch_con, con_mask=_batch_mask, batch_img_clip=_batch_img_ori, \
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/diffusion/flow_matching.py", line 339, in forward
    return self.p_losses_textVAE(
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/diffusion/flow_matching.py", line 424, in p_losses_textVAE
    prediction = nnet(x_noisy, log_snr = log_snr, null_indicator=null_indicator)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/accelerate/utils/operations.py", line 507, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast
    return func(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 455, in forward
    return self._forward(images = x, log_snr = log_snr, null_indicator=null_indicator)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 400, in _forward
    output, lowres_skips = stage(images, lowres_skips = lowres_skips, condition_context = condition_context, condition_embeds = condition_embeds, condition_masks = condition_masks, null_indicator=null_indicator)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 288, in forward
    x = block(x, condition_embeds, condition_masks, pos=pos)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 172, in forward
    return torch.utils.checkpoint.checkpoint(self._forward, x, condition_embeds, condition_masks, pos)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/_compile.py", line 24, in inner
    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py", line 328, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/_dynamo/external_utils.py", line 17, in inner
    return fn(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 451, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 230, in forward
    outputs = run_function(*args)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 178, in _forward
    x = x + self.dropout(self.block2(x))
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 96, in forward
    x = self.act(self.w0(x)) * self.w1(x)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
Traceback (most recent call last):
  File "train_t2i.py", line 362, in <module>
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 1.38 MiB is free. Process 1479858 has 31.73 GiB memory in use. Of the allocated memory 30.99 GiB is allocated by PyTorch, and 24.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    app.run(main)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/absl/app.py", line 316, in run
    _run_main(main, args)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/absl/app.py", line 261, in _run_main
    sys.exit(main(argv))
  File "train_t2i.py", line 358, in main
    train(config)
  File "train_t2i.py", line 259, in train
    metrics = train_step(batch, ss_empty_context)
  File "train_t2i.py", line 174, in train_step
    loss, loss_dict = _flow_mathcing_model(_z, nnet, loss_coeffs=config.loss_coeffs, cond=_batch_con, con_mask=_batch_mask, batch_img_clip=_batch_img_ori, \
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/diffusion/flow_matching.py", line 339, in forward
    return self.p_losses_textVAE(
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/diffusion/flow_matching.py", line 424, in p_losses_textVAE
    prediction = nnet(x_noisy, log_snr = log_snr, null_indicator=null_indicator)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/accelerate/utils/operations.py", line 507, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast
    return func(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 455, in forward
    return self._forward(images = x, log_snr = log_snr, null_indicator=null_indicator)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 400, in _forward
    output, lowres_skips = stage(images, lowres_skips = lowres_skips, condition_context = condition_context, condition_embeds = condition_embeds, condition_masks = condition_masks, null_indicator=null_indicator)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
Traceback (most recent call last):
  File "train_t2i.py", line 362, in <module>
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 287, in forward
    x = dense(torch.cat([x, skips.pop()], dim = -1))
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
        return self._call_impl(*args, **kwargs)
app.run(main)  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl

  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/absl/app.py", line 316, in run
    _run_main(main, args)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/absl/app.py", line 261, in _run_main
    sys.exit(main(argv))    
return forward_call(*args, **kwargs)
  File "train_t2i.py", line 358, in main
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 25.38 MiB is free. Process 1479857 has 31.71 GiB memory in use. Of the allocated memory 30.95 GiB is allocated by PyTorch, and 19.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    train(config)
  File "train_t2i.py", line 259, in train
    metrics = train_step(batch, ss_empty_context)
  File "train_t2i.py", line 174, in train_step
    loss, loss_dict = _flow_mathcing_model(_z, nnet, loss_coeffs=config.loss_coeffs, cond=_batch_con, con_mask=_batch_mask, batch_img_clip=_batch_img_ori, \
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/diffusion/flow_matching.py", line 339, in forward
    return self.p_losses_textVAE(
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/diffusion/flow_matching.py", line 424, in p_losses_textVAE
    prediction = nnet(x_noisy, log_snr = log_snr, null_indicator=null_indicator)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/accelerate/utils/operations.py", line 507, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast
    return func(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 455, in forward
    return self._forward(images = x, log_snr = log_snr, null_indicator=null_indicator)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 400, in _forward
    output, lowres_skips = stage(images, lowres_skips = lowres_skips, condition_context = condition_context, condition_embeds = condition_embeds, condition_masks = condition_masks, null_indicator=null_indicator)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 288, in forward
    x = block(x, condition_embeds, condition_masks, pos=pos)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 172, in forward
    return torch.utils.checkpoint.checkpoint(self._forward, x, condition_embeds, condition_masks, pos)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/_compile.py", line 24, in inner
    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py", line 328, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/_dynamo/external_utils.py", line 17, in inner
    return fn(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 451, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 230, in forward
    outputs = run_function(*args)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 177, in _forward
    x = x + self.dropout(self.block1(x, condition_embeds, condition_masks, pos))
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 136, in forward
    qkv = self.query_key_value(self.norm(x))
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 15.38 MiB is free. Process 1479856 has 31.72 GiB memory in use. Of the allocated memory 30.99 GiB is allocated by PyTorch, and 20.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_t2i.py", line 362, in <module>
    app.run(main)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/absl/app.py", line 316, in run
    _run_main(main, args)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/absl/app.py", line 261, in _run_main
    sys.exit(main(argv))
  File "train_t2i.py", line 358, in main
    train(config)
  File "train_t2i.py", line 259, in train
    metrics = train_step(batch, ss_empty_context)
  File "train_t2i.py", line 174, in train_step
    loss, loss_dict = _flow_mathcing_model(_z, nnet, loss_coeffs=config.loss_coeffs, cond=_batch_con, con_mask=_batch_mask, batch_img_clip=_batch_img_ori, \
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/diffusion/flow_matching.py", line 339, in forward
    return self.p_losses_textVAE(
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/diffusion/flow_matching.py", line 424, in p_losses_textVAE
    prediction = nnet(x_noisy, log_snr = log_snr, null_indicator=null_indicator)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/accelerate/utils/operations.py", line 507, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast
    return func(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 455, in forward
    return self._forward(images = x, log_snr = log_snr, null_indicator=null_indicator)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 400, in _forward
    output, lowres_skips = stage(images, lowres_skips = lowres_skips, condition_context = condition_context, condition_embeds = condition_embeds, condition_masks = condition_masks, null_indicator=null_indicator)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 288, in forward
    x = block(x, condition_embeds, condition_masks, pos=pos)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 172, in forward
    return torch.utils.checkpoint.checkpoint(self._forward, x, condition_embeds, condition_masks, pos)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/_compile.py", line 24, in inner
    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py", line 328, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/_dynamo/external_utils.py", line 17, in inner
    return fn(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 451, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 230, in forward
    outputs = run_function(*args)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 177, in _forward
    x = x + self.dropout(self.block1(x, condition_embeds, condition_masks, pos))
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 136, in forward
    qkv = self.query_key_value(self.norm(x))
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 3.38 MiB is free. Process 1479855 has 31.73 GiB memory in use. Of the allocated memory 30.99 GiB is allocated by PyTorch, and 20.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
