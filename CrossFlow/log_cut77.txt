The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_cpu_threads_per_process` was set to `5` to improve out-of-box performance
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/opt/conda/envs/crossflow/lib/python3.8/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/opt/conda/envs/crossflow/lib/python3.8/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/opt/conda/envs/crossflow/lib/python3.8/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/opt/conda/envs/crossflow/lib/python3.8/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
I0412 16:19:27.008412 23263182075584 train_t2i.py:42] Process 0 using device: cuda:0
I0412 16:19:27.009707 22552225937088 train_t2i.py:42] Process 1 using device: cuda:1
4ebf64dcfb814391b20452c5eacea59200001W:692745:692745 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
4ebf64dcfb814391b20452c5eacea59200001W:692745:692745 [0] NCCL INFO Bootstrap : Using eth0:10.0.0.7<0>
4ebf64dcfb814391b20452c5eacea59200001W:692745:692745 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
4ebf64dcfb814391b20452c5eacea59200001W:692745:692745 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
4ebf64dcfb814391b20452c5eacea59200001W:692745:692745 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.18.6+cuda12.1
4ebf64dcfb814391b20452c5eacea59200001W:692746:692746 [1] NCCL INFO cudaDriverVersion 12020
4ebf64dcfb814391b20452c5eacea59200001W:692746:692746 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
4ebf64dcfb814391b20452c5eacea59200001W:692746:692746 [1] NCCL INFO Bootstrap : Using eth0:10.0.0.7<0>
4ebf64dcfb814391b20452c5eacea59200001W:692746:692746 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
4ebf64dcfb814391b20452c5eacea59200001W:692746:692746 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
I0412 16:19:27.377703 22538134849216 train_t2i.py:42] Process 3 using device: cuda:3
4ebf64dcfb814391b20452c5eacea59200001W:692748:692748 [3] NCCL INFO cudaDriverVersion 12020
4ebf64dcfb814391b20452c5eacea59200001W:692748:692748 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
4ebf64dcfb814391b20452c5eacea59200001W:692748:692748 [3] NCCL INFO Bootstrap : Using eth0:10.0.0.7<0>
4ebf64dcfb814391b20452c5eacea59200001W:692748:692748 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
4ebf64dcfb814391b20452c5eacea59200001W:692748:692748 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
I0412 16:19:27.396200 22942584932032 train_t2i.py:42] Process 2 using device: cuda:2
4ebf64dcfb814391b20452c5eacea59200001W:692747:692747 [2] NCCL INFO cudaDriverVersion 12020
4ebf64dcfb814391b20452c5eacea59200001W:692747:692747 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
4ebf64dcfb814391b20452c5eacea59200001W:692747:692747 [2] NCCL INFO Bootstrap : Using eth0:10.0.0.7<0>
4ebf64dcfb814391b20452c5eacea59200001W:692747:692747 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
4ebf64dcfb814391b20452c5eacea59200001W:692747:692747 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins/lib/libnccl-net.so
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO P2P plugin IBext
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB [RO]; OOB eth0:10.0.0.7<0>
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO Using network IBext
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins/lib/libnccl-net.so
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO P2P plugin IBext
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB [RO]; OOB eth0:10.0.0.7<0>
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO Using network IBext
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins/lib/libnccl-net.so
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO P2P plugin IBext
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB [RO]; OOB eth0:10.0.0.7<0>
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Using network IBext
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins/lib/libnccl-net.so
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO P2P plugin IBext
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO NET/IB : Using [0]mlx5_ib0:1/IB [RO]; OOB eth0:10.0.0.7<0>
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Using network IBext
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO comm 0xa4cd550 rank 3 nranks 4 cudaDev 3 nvmlDev 7 busId 800000 commId 0xf64efd25e0bafe7b - Init START
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO comm 0xacd4190 rank 2 nranks 4 cudaDev 2 nvmlDev 6 busId 700000 commId 0xf64efd25e0bafe7b - Init START
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO comm 0xafe7320 rank 1 nranks 4 cudaDev 1 nvmlDev 5 busId 600000 commId 0xf64efd25e0bafe7b - Init START
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO comm 0xb1f6ae0 rank 0 nranks 4 cudaDev 0 nvmlDev 4 busId 500000 commId 0xf64efd25e0bafe7b - Init START
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO Setting affinity for GPU 5 to ff,fff00000
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO NVLS multicast support is not available on dev 1
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Setting affinity for GPU 6 to ff,fff00000
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO NVLS multicast support is not available on dev 2
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Setting affinity for GPU 7 to ff,fff00000
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO NVLS multicast support is not available on dev 3
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO Setting affinity for GPU 4 to ff,fff00000
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO NVLS multicast support is not available on dev 0
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] -1/-1/-1->1->2 [2] -1/-1/-1->1->2 [3] -1/-1/-1->1->3 [4] 2/-1/-1->1->0 [5] -1/-1/-1->1->2 [6] -1/-1/-1->1->2 [7] -1/-1/-1->1->3
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO Channel 00/08 :    0   1   2   3
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO Channel 01/08 :    0   3   2   1
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO P2P Chunksize set to 524288
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO Channel 02/08 :    0   3   1   2
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO Channel 03/08 :    0   2   1   3
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO Channel 04/08 :    0   1   2   3
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO Channel 05/08 :    0   3   2   1
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO Channel 06/08 :    0   3   1   2
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO Channel 07/08 :    0   2   1   3
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 3/-1/-1->0->-1 [2] 3/-1/-1->0->-1 [3] 2/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 3/-1/-1->0->-1 [6] 3/-1/-1->0->-1 [7] 2/-1/-1->0->-1
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 1/-1/-1->2->3 [2] 1/-1/-1->2->3 [3] 3/-1/-1->2->0 [4] 3/-1/-1->2->1 [5] 1/-1/-1->2->3 [6] 1/-1/-1->2->3 [7] 3/-1/-1->2->0
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO P2P Chunksize set to 524288
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 2/-1/-1->3->0 [2] 2/-1/-1->3->0 [3] 1/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] 2/-1/-1->3->0 [6] 2/-1/-1->3->0 [7] 1/-1/-1->3->2
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO P2P Chunksize set to 524288
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO P2P Chunksize set to 524288
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO Channel 00/0 : 1[5] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO Channel 02/0 : 1[5] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO Channel 00/0 : 0[4] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO Channel 04/0 : 1[5] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO Channel 04/0 : 0[4] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO Channel 06/0 : 1[5] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Channel 00/0 : 3[7] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Channel 03/0 : 3[7] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Channel 00/0 : 2[6] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Channel 04/0 : 3[7] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Channel 04/0 : 2[6] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Channel 07/0 : 3[7] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO Channel 03/0 : 1[5] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Channel 02/0 : 2[6] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO Channel 03/0 : 0[4] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Channel 02/0 : 3[7] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO Channel 07/0 : 1[5] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Channel 06/0 : 2[6] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO Channel 07/0 : 0[4] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Channel 06/0 : 3[7] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO Channel 01/0 : 0[4] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Channel 01/0 : 2[6] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO Channel 02/0 : 0[4] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Channel 03/0 : 2[6] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO Channel 05/0 : 0[4] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Channel 05/0 : 2[6] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO Channel 01/0 : 1[5] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Channel 01/0 : 3[7] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO Channel 06/0 : 0[4] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Channel 07/0 : 2[6] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO Channel 05/0 : 1[5] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Channel 05/0 : 3[7] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO Connected all rings
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO Connected all rings
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO Channel 01/0 : 1[5] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Connected all rings
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Connected all rings
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO Channel 05/0 : 1[5] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Channel 01/0 : 2[6] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Channel 02/0 : 2[6] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Channel 03/0 : 2[6] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Channel 05/0 : 2[6] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Channel 06/0 : 2[6] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Channel 01/0 : 3[7] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Channel 07/0 : 2[6] -> 3[7] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Channel 02/0 : 3[7] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Channel 05/0 : 3[7] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Channel 06/0 : 3[7] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Channel 03/0 : 2[6] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Channel 03/0 : 3[7] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Channel 07/0 : 2[6] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Channel 07/0 : 3[7] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Channel 00/0 : 3[7] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Channel 02/0 : 3[7] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Channel 03/0 : 3[7] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Channel 04/0 : 3[7] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO Channel 00/0 : 1[5] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Channel 06/0 : 3[7] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO Channel 04/0 : 1[5] -> 0[4] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Channel 07/0 : 3[7] -> 2[6] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Channel 00/0 : 2[6] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Channel 02/0 : 2[6] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Channel 04/0 : 2[6] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Channel 06/0 : 2[6] -> 1[5] via P2P/IPC
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO Connected all trees
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO Connected all trees
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO Connected all trees
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO Connected all trees
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO 8 coll channels, 0 nvls channels, 8 p2p channels, 2 p2p channels per peer
4ebf64dcfb814391b20452c5eacea59200001W:692748:692815 [3] NCCL INFO comm 0xa4cd550 rank 3 nranks 4 cudaDev 3 nvmlDev 7 busId 800000 commId 0xf64efd25e0bafe7b - Init COMPLETE
4ebf64dcfb814391b20452c5eacea59200001W:692746:692813 [1] NCCL INFO comm 0xafe7320 rank 1 nranks 4 cudaDev 1 nvmlDev 5 busId 600000 commId 0xf64efd25e0bafe7b - Init COMPLETE
4ebf64dcfb814391b20452c5eacea59200001W:692745:692812 [0] NCCL INFO comm 0xb1f6ae0 rank 0 nranks 4 cudaDev 0 nvmlDev 4 busId 500000 commId 0xf64efd25e0bafe7b - Init COMPLETE
4ebf64dcfb814391b20452c5eacea59200001W:692747:692817 [2] NCCL INFO comm 0xacd4190 rank 2 nranks 4 cudaDev 2 nvmlDev 6 busId 700000 commId 0xf64efd25e0bafe7b - Init COMPLETE
/opt/conda/envs/crossflow/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
/opt/conda/envs/crossflow/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/opt/conda/envs/crossflow/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
wandb: Tracking run with wandb version 0.19.9
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
2025-04-12 16:19:30,366 - train_t2i.py - autoencoder:
  pretrained_path: /storage/v-jinpewang/lab_folder/qisheng_data/assets/stable-diffusion/autoencoder_kl.pth
  scale_factor: 0.2301
ckpt_root: /storage/v-jinpewang/lab_folder/qisheng_data/workdir/t2i_training_demo/default/ckpts
config_name: t2i_training_demo
dataset:
  cfg: false
  llm: t5
  name: textimage_features
  resolution: 256
  train_path: /storage/v-jinpewang/lab_folder/qisheng_data/raw_text_image_dataset_f10px512
  val_path: /storage/v-jinpewang/lab_folder/qisheng_data/raw_text_image_testset_f10px512
hparams: default
loss_coeffs: !!python/tuple
- 0.25
- 1
lr_scheduler:
  name: customized
  warmup_steps: 5000
mixed_precision: fp16
nnet:
  model_args: !!python/object:config_config.Args
    adapter_in_embed: 2048
    block_grad_to_lowres: false
    cfg_indicator: 0.1
    channels: 4
    clip_dim: 768
    gradient_checking: true
    norm_type: TDRMSN
    num_clip_token: 77
    stage_configs:
    - !!python/object:config_config.Args
      block_type: TransformerBlock
      dim: 1024
      dropout_prob: 0
      final_kernel_size: 3
      hidden_dim: 2048
      image_input_ratio: 1
      input_feature_ratio: 2
      max_height: 16
      max_width: 16
      num_attention_heads: 16
      num_blocks: 65
    - !!python/object:config_config.Args
      block_type: ConvNeXtBlock
      dim: 512
      dropout_prob: 0
      final_kernel_size: 3
      hidden_dim: 1024
      image_input_ratio: 1
      input_feature_ratio: 1
      kernel_size: 7
      max_height: 32
      max_width: 32
      num_blocks: 33
    textVAE: !!python/object:config_config.Args
      dropout_prob: 0.1
      hidden_dim: 1024
      hidden_token_length: 256
      num_attention_heads: 8
      num_blocks: 11
    use_t2i: true
  name: dimr
optimizer:
  betas: !!python/tuple
  - 0.9
  - 0.9
  lr: 1.0e-05
  name: adamw
  weight_decay: 0.03
pretrained_path: /storage/v-jinpewang/lab_folder/qisheng_data/t2i_256px_clip_dimr.pth
sample:
  cfg: false
  mini_batch_size: 5
  n_samples: 500
  path: /storage/v-jinpewang/lab_folder/qisheng_data/samplesave
  sample_steps: 50
  scale: 7
sample_dir: /storage/v-jinpewang/lab_folder/qisheng_data/workdir/t2i_training_demo/default/samples
seed: 1234
train:
  batch_size: 4
  eval_interval: 20000
  log_interval: 20000
  mode: cond
  n_samples_eval: 5
  n_steps: 800000
  save_interval: 20000
workdir: /storage/v-jinpewang/lab_folder/qisheng_data/workdir/t2i_training_demo/default
z_shape: !!python/tuple
- 4
- 32
- 32

2025-04-12 16:19:30,376 - train_t2i.py - Run on 4 devices
Prepare dataset...
Prepare dataset ok
/opt/conda/envs/crossflow/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
TransEncoder no compressor!
2025-04-12 16:19:43,780 - factory.py - Loaded ViT-L-16-SigLIP-256 model config.
成功加载基础参数，检测到 0 个adapter参数需要初始化
TransEncoder no compressor!
2025-04-12 16:20:08,715 - factory.py - Loaded ViT-L-16-SigLIP-256 model config.
params_decay 1846
params_nodecay 342
train_state.nnet: MRModel(
  (stages): ModuleList(
    (0): Stage(
      (input_layer): Sequential(
        (0): DownInterpolate(ratio=1)
        (1): Conv2d(4, 1024, kernel_size=(2, 2), stride=(2, 2))
        (2): ChannelLast()
        (3): PositionEmbeddings(
          (position_embeddings): Embedding(256, 1024)
        )
      )
      (blocks): ModuleList(
        (0-64): 65 x TransformerBlock(
          (block1): SelfAttention(
            (norm): TDRMSNorm(
              (scale): TimeDependentParameter(shape=(1024,))
            )
            (query_key_value): Linear(in_features=1024, out_features=3072, bias=False)
            (dense): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (block2): MLPBlock(
            (norm): TDRMSNorm(
              (scale): TimeDependentParameter(shape=(1024,))
            )
            (act): GELU(approximate='none')
            (w0): Linear(in_features=1024, out_features=2048, bias=True)
            (w1): Linear(in_features=1024, out_features=2048, bias=True)
            (w2): Linear(in_features=2048, out_features=1024, bias=True)
          )
          (dropout): Dropout(p=0, inplace=False)
        )
      )
      (skip_denses): ModuleList(
        (0-31): 32 x Linear(in_features=2048, out_features=1024, bias=True)
      )
      (output_layer): Sequential(
        (0): LayerNorm(
          (weight): TimeDependentParameter(shape=(1024,))
          (bias): TimeDependentParameter(shape=(1024,))
        )
        (1): ChannelFirst()
        (2): Conv2d(1024, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (1): Stage(
      (input_layer): Sequential(
        (0): DownInterpolate(ratio=1)
        (1): Conv2d(4, 512, kernel_size=(1, 1), stride=(1, 1))
        (2): ChannelLast()
        (3): PositionEmbeddings(
          (position_embeddings): Embedding(1024, 512)
        )
      )
      (upsample): Sequential(
        (0): LayerNorm(
          (weight): TimeDependentParameter(shape=(67584,))
          (bias): TimeDependentParameter(shape=(67584,))
        )
        (1): PixelShuffleUpsample(
          (kernel): Linear(in_features=67584, out_features=2048, bias=True)
        )
        (2): LayerNorm(
          (weight): TimeDependentParameter(shape=(512,))
          (bias): TimeDependentParameter(shape=(512,))
        )
      )
      (blocks): ModuleList(
        (0-32): 33 x ConvNeXtBlock(
          (block1): Sequential(
            (0): ChannelFirst()
            (1): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
            (2): ChannelLast()
          )
          (block2): MLPBlock(
            (norm): TDRMSNorm(
              (scale): TimeDependentParameter(shape=(512,))
            )
            (act): GELU(approximate='none')
            (w0): Linear(in_features=512, out_features=1024, bias=True)
            (w1): Linear(in_features=512, out_features=1024, bias=True)
            (w2): Linear(in_features=1024, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0, inplace=False)
        )
      )
      (skip_denses): ModuleList(
        (0-15): 16 x Linear(in_features=1024, out_features=512, bias=True)
      )
      (output_layer): Sequential(
        (0): LayerNorm(
          (weight): TimeDependentParameter(shape=(512,))
          (bias): TimeDependentParameter(shape=(512,))
        )
        (1): ChannelFirst()
        (2): Conv2d(512, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (context_encoder): TransEncoder(
    (layers): ModuleList(
      (0-10): 11 x EncoderLayer(
        (attn): MultiHeadAttentioin(
          (W_Q): Linear(in_features=768, out_features=768, bias=True)
          (W_K): Linear(in_features=768, out_features=768, bias=True)
          (W_V): Linear(in_features=768, out_features=768, bias=True)
          (W_O): Linear(in_features=768, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): FeedForward(
          (ffn_1): Linear(in_features=768, out_features=1024, bias=True)
          (ffn_2): Linear(in_features=1024, out_features=768, bias=True)
          (act): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (norm1): LayerNorm()
        (norm2): LayerNorm()
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (reduction_layers): ModuleList(
      (0): EncoderReductionLayer(
        (attn): MultiHeadAttentioin(
          (W_Q): Linear(in_features=768, out_features=768, bias=True)
          (W_K): Linear(in_features=768, out_features=768, bias=True)
          (W_V): Linear(in_features=768, out_features=768, bias=True)
          (W_O): Linear(in_features=768, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): FeedForward(
          (ffn_1): Linear(in_features=768, out_features=1024, bias=True)
          (ffn_2): Linear(in_features=1024, out_features=768, bias=True)
          (act): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (reduction): Linear(in_features=768, out_features=384, bias=True)
        (norm1): LayerNorm()
        (norm2): LayerNorm()
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderReductionLayer(
        (attn): MultiHeadAttentioin(
          (W_Q): Linear(in_features=384, out_features=384, bias=True)
          (W_K): Linear(in_features=384, out_features=384, bias=True)
          (W_V): Linear(in_features=384, out_features=384, bias=True)
          (W_O): Linear(in_features=384, out_features=384, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): FeedForward(
          (ffn_1): Linear(in_features=384, out_features=1024, bias=True)
          (ffn_2): Linear(in_features=1024, out_features=384, bias=True)
          (act): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (reduction): Linear(in_features=384, out_features=192, bias=True)
        (norm1): LayerNorm()
        (norm2): LayerNorm()
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderReductionLayer(
        (attn): MultiHeadAttentioin(
          (W_Q): Linear(in_features=192, out_features=192, bias=True)
          (W_K): Linear(in_features=192, out_features=192, bias=True)
          (W_V): Linear(in_features=192, out_features=192, bias=True)
          (W_O): Linear(in_features=192, out_features=192, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): FeedForward(
          (ffn_1): Linear(in_features=192, out_features=1024, bias=True)
          (ffn_2): Linear(in_features=1024, out_features=192, bias=True)
          (act): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (reduction): Linear(in_features=192, out_features=96, bias=True)
        (norm1): LayerNorm()
        (norm2): LayerNorm()
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (linear): Linear(in_features=7392, out_features=8192, bias=True)
  )
  (open_clip): CustomTextCLIP(
    (visual): TimmModel(
      (trunk): VisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (pos_drop): Dropout(p=0.0, inplace=False)
        (patch_drop): Identity()
        (norm_pre): Identity()
        (blocks): Sequential(
          (0): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (1): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (2): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (3): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (4): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (5): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (6): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (7): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (8): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (9): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (10): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (11): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (12): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (13): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (14): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (15): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (16): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (17): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (18): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (19): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (20): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (21): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (22): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (23): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn_pool): AttentionPoolLatent(
          (q): Linear(in_features=1024, out_features=1024, bias=True)
          (kv): Linear(in_features=1024, out_features=2048, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (fc_norm): Identity()
        (head_drop): Dropout(p=0.0, inplace=False)
        (head): Identity()
      )
      (head): Sequential()
    )
  )
  (open_clip_output): Mlp(
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (act): GELU(approximate='none')
    (drop1): Dropout(p=0.0, inplace=False)
    (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
    (fc2): Linear(in_features=4096, out_features=4096, bias=True)
    (drop2): Dropout(p=0.0, inplace=False)
  )
)
train_state.nnet_ema: MRModel(
  (stages): ModuleList(
    (0): Stage(
      (input_layer): Sequential(
        (0): DownInterpolate(ratio=1)
        (1): Conv2d(4, 1024, kernel_size=(2, 2), stride=(2, 2))
        (2): ChannelLast()
        (3): PositionEmbeddings(
          (position_embeddings): Embedding(256, 1024)
        )
      )
      (blocks): ModuleList(
        (0-64): 65 x TransformerBlock(
          (block1): SelfAttention(
            (norm): TDRMSNorm(
              (scale): TimeDependentParameter(shape=(1024,))
            )
            (query_key_value): Linear(in_features=1024, out_features=3072, bias=False)
            (dense): Linear(in_features=1024, out_features=1024, bias=True)
          )
          (block2): MLPBlock(
            (norm): TDRMSNorm(
              (scale): TimeDependentParameter(shape=(1024,))
            )
            (act): GELU(approximate='none')
            (w0): Linear(in_features=1024, out_features=2048, bias=True)
            (w1): Linear(in_features=1024, out_features=2048, bias=True)
            (w2): Linear(in_features=2048, out_features=1024, bias=True)
          )
          (dropout): Dropout(p=0, inplace=False)
        )
      )
      (skip_denses): ModuleList(
        (0-31): 32 x Linear(in_features=2048, out_features=1024, bias=True)
      )
      (output_layer): Sequential(
        (0): LayerNorm(
          (weight): TimeDependentParameter(shape=(1024,))
          (bias): TimeDependentParameter(shape=(1024,))
        )
        (1): ChannelFirst()
        (2): Conv2d(1024, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (1): Stage(
      (input_layer): Sequential(
        (0): DownInterpolate(ratio=1)
        (1): Conv2d(4, 512, kernel_size=(1, 1), stride=(1, 1))
        (2): ChannelLast()
        (3): PositionEmbeddings(
          (position_embeddings): Embedding(1024, 512)
        )
      )
      (upsample): Sequential(
        (0): LayerNorm(
          (weight): TimeDependentParameter(shape=(67584,))
          (bias): TimeDependentParameter(shape=(67584,))
        )
        (1): PixelShuffleUpsample(
          (kernel): Linear(in_features=67584, out_features=2048, bias=True)
        )
        (2): LayerNorm(
          (weight): TimeDependentParameter(shape=(512,))
          (bias): TimeDependentParameter(shape=(512,))
        )
      )
      (blocks): ModuleList(
        (0-32): 33 x ConvNeXtBlock(
          (block1): Sequential(
            (0): ChannelFirst()
            (1): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
            (2): ChannelLast()
          )
          (block2): MLPBlock(
            (norm): TDRMSNorm(
              (scale): TimeDependentParameter(shape=(512,))
            )
            (act): GELU(approximate='none')
            (w0): Linear(in_features=512, out_features=1024, bias=True)
            (w1): Linear(in_features=512, out_features=1024, bias=True)
            (w2): Linear(in_features=1024, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0, inplace=False)
        )
      )
      (skip_denses): ModuleList(
        (0-15): 16 x Linear(in_features=1024, out_features=512, bias=True)
      )
      (output_layer): Sequential(
        (0): LayerNorm(
          (weight): TimeDependentParameter(shape=(512,))
          (bias): TimeDependentParameter(shape=(512,))
        )
        (1): ChannelFirst()
        (2): Conv2d(512, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (context_encoder): TransEncoder(
    (layers): ModuleList(
      (0-10): 11 x EncoderLayer(
        (attn): MultiHeadAttentioin(
          (W_Q): Linear(in_features=768, out_features=768, bias=True)
          (W_K): Linear(in_features=768, out_features=768, bias=True)
          (W_V): Linear(in_features=768, out_features=768, bias=True)
          (W_O): Linear(in_features=768, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): FeedForward(
          (ffn_1): Linear(in_features=768, out_features=1024, bias=True)
          (ffn_2): Linear(in_features=1024, out_features=768, bias=True)
          (act): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (norm1): LayerNorm()
        (norm2): LayerNorm()
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (reduction_layers): ModuleList(
      (0): EncoderReductionLayer(
        (attn): MultiHeadAttentioin(
          (W_Q): Linear(in_features=768, out_features=768, bias=True)
          (W_K): Linear(in_features=768, out_features=768, bias=True)
          (W_V): Linear(in_features=768, out_features=768, bias=True)
          (W_O): Linear(in_features=768, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): FeedForward(
          (ffn_1): Linear(in_features=768, out_features=1024, bias=True)
          (ffn_2): Linear(in_features=1024, out_features=768, bias=True)
          (act): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (reduction): Linear(in_features=768, out_features=384, bias=True)
        (norm1): LayerNorm()
        (norm2): LayerNorm()
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderReductionLayer(
        (attn): MultiHeadAttentioin(
          (W_Q): Linear(in_features=384, out_features=384, bias=True)
          (W_K): Linear(in_features=384, out_features=384, bias=True)
          (W_V): Linear(in_features=384, out_features=384, bias=True)
          (W_O): Linear(in_features=384, out_features=384, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): FeedForward(
          (ffn_1): Linear(in_features=384, out_features=1024, bias=True)
          (ffn_2): Linear(in_features=1024, out_features=384, bias=True)
          (act): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (reduction): Linear(in_features=384, out_features=192, bias=True)
        (norm1): LayerNorm()
        (norm2): LayerNorm()
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderReductionLayer(
        (attn): MultiHeadAttentioin(
          (W_Q): Linear(in_features=192, out_features=192, bias=True)
          (W_K): Linear(in_features=192, out_features=192, bias=True)
          (W_V): Linear(in_features=192, out_features=192, bias=True)
          (W_O): Linear(in_features=192, out_features=192, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): FeedForward(
          (ffn_1): Linear(in_features=192, out_features=1024, bias=True)
          (ffn_2): Linear(in_features=1024, out_features=192, bias=True)
          (act): ReLU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (reduction): Linear(in_features=192, out_features=96, bias=True)
        (norm1): LayerNorm()
        (norm2): LayerNorm()
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (linear): Linear(in_features=7392, out_features=8192, bias=True)
  )
  (open_clip): CustomTextCLIP(
    (visual): TimmModel(
      (trunk): VisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
          (norm): Identity()
        )
        (pos_drop): Dropout(p=0.0, inplace=False)
        (patch_drop): Identity()
        (norm_pre): Identity()
        (blocks): Sequential(
          (0): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (1): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (2): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (3): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (4): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (5): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (6): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (7): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (8): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (9): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (10): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (11): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (12): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (13): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (14): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (15): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (16): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (17): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (18): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (19): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (20): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (21): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (22): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (23): Block(
            (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (q_norm): Identity()
              (k_norm): Identity()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=1024, out_features=4096, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (norm): Identity()
              (fc2): Linear(in_features=4096, out_features=1024, bias=True)
              (drop2): Dropout(p=0.0, inplace=False)
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
        )
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (attn_pool): AttentionPoolLatent(
          (q): Linear(in_features=1024, out_features=1024, bias=True)
          (kv): Linear(in_features=1024, out_features=2048, bias=True)
          (q_norm): Identity()
          (k_norm): Identity()
          (proj): Linear(in_features=1024, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (fc_norm): Identity()
        (head_drop): Dropout(p=0.0, inplace=False)
        (head): Identity()
      )
      (head): Sequential()
    )
  )
  (open_clip_output): Mlp(
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (act): GELU(approximate='none')
    (drop1): Dropout(p=0.0, inplace=False)
    (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
    (fc2): Linear(in_features=4096, out_features=4096, bias=True)
    (drop2): Dropout(p=0.0, inplace=False)
  )
)
Create autoencoder with scale_factor=0.2301
making attention of type 'vanilla' with 512 in_channels
Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
making attention of type 'vanilla' with 512 in_channels
Using Janus-Pro-1B
2025-04-12 16:20:26,885 - train_t2i.py - Start fitting, step=0, mixed_precision=fp16
epoch:   0%|          | 0/2500 [00:00<?, ?it/s]/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/sigmoid/kernel.py:30: UserWarning: Using slower tdp_torch implementation for a tensor with shape torch.Size([1024])
  warnings.warn(f'Using slower tdp_torch implementation for a tensor with shape {param0.shape}')
/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/sigmoid/kernel.py:30: UserWarning: Using slower tdp_torch implementation for a tensor with shape torch.Size([67584])
  warnings.warn(f'Using slower tdp_torch implementation for a tensor with shape {param0.shape}')
/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/sigmoid/kernel.py:30: UserWarning: Using slower tdp_torch implementation for a tensor with shape torch.Size([512])
  warnings.warn(f'Using slower tdp_torch implementation for a tensor with shape {param0.shape}')
/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/sigmoid/kernel.py:30: UserWarning: Using slower tdp_torch implementation for a tensor with shape torch.Size([1024])
  warnings.warn(f'Using slower tdp_torch implementation for a tensor with shape {param0.shape}')
/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/sigmoid/kernel.py:30: UserWarning: Using slower tdp_torch implementation for a tensor with shape torch.Size([67584])
  warnings.warn(f'Using slower tdp_torch implementation for a tensor with shape {param0.shape}')
/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/sigmoid/kernel.py:30: UserWarning: Using slower tdp_torch implementation for a tensor with shape torch.Size([512])
  warnings.warn(f'Using slower tdp_torch implementation for a tensor with shape {param0.shape}')
/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/sigmoid/kernel.py:30: UserWarning: Using slower tdp_torch implementation for a tensor with shape torch.Size([1024])
  warnings.warn(f'Using slower tdp_torch implementation for a tensor with shape {param0.shape}')
/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/sigmoid/kernel.py:30: UserWarning: Using slower tdp_torch implementation for a tensor with shape torch.Size([67584])
  warnings.warn(f'Using slower tdp_torch implementation for a tensor with shape {param0.shape}')
/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/sigmoid/kernel.py:30: UserWarning: Using slower tdp_torch implementation for a tensor with shape torch.Size([512])
  warnings.warn(f'Using slower tdp_torch implementation for a tensor with shape {param0.shape}')
/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/sigmoid/kernel.py:30: UserWarning: Using slower tdp_torch implementation for a tensor with shape torch.Size([1024])
  warnings.warn(f'Using slower tdp_torch implementation for a tensor with shape {param0.shape}')
/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/sigmoid/kernel.py:30: UserWarning: Using slower tdp_torch implementation for a tensor with shape torch.Size([67584])
  warnings.warn(f'Using slower tdp_torch implementation for a tensor with shape {param0.shape}')
/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/sigmoid/kernel.py:30: UserWarning: Using slower tdp_torch implementation for a tensor with shape torch.Size([512])
  warnings.warn(f'Using slower tdp_torch implementation for a tensor with shape {param0.shape}')
/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [512, 1, 7, 7], strides() = [49, 1, 7, 1]
bucket_view.sizes() = [512, 1, 7, 7], strides() = [49, 49, 7, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [512, 1, 7, 7], strides() = [49, 1, 7, 1]
bucket_view.sizes() = [512, 1, 7, 7], strides() = [49, 49, 7, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [512, 1, 7, 7], strides() = [49, 1, 7, 1]
bucket_view.sizes() = [512, 1, 7, 7], strides() = [49, 49, 7, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/autograd/__init__.py:251: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [512, 1, 7, 7], strides() = [49, 1, 7, 1]
bucket_view.sizes() = [512, 1, 7, 7], strides() = [49, 49, 7, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:320.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
epoch:   0%|          | 1/2500 [00:53<37:14:46, 53.66s/it]epoch:   0%|          | 2/2500 [00:55<16:05:00, 23.18s/it]epoch:   0%|          | 3/2500 [00:56<8:59:55, 12.97s/it] epoch:   0%|          | 4/2500 [00:57<5:40:08,  8.18s/it]epoch:   0%|          | 5/2500 [00:57<3:49:26,  5.52s/it]epoch:   0%|          | 6/2500 [00:59<2:48:14,  4.05s/it]Traceback (most recent call last):
  File "train_t2i.py", line 362, in <module>
    app.run(main)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/absl/app.py", line 316, in run
    _run_main(main, args)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/absl/app.py", line 261, in _run_main
    sys.exit(main(argv))
  File "train_t2i.py", line 358, in main
    train(config)
  File "train_t2i.py", line 259, in train
    metrics = train_step(batch, ss_empty_context)
  File "train_t2i.py", line 174, in train_step
    loss, loss_dict = _flow_mathcing_model(_z, nnet, loss_coeffs=config.loss_coeffs, cond=_batch_con, con_mask=_batch_mask, batch_img_clip=_batch_img_ori, \
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/diffusion/flow_matching.py", line 339, in forward
    return self.p_losses_textVAE(
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/diffusion/flow_matching.py", line 424, in p_losses_textVAE
    prediction = nnet(x_noisy, log_snr = log_snr, null_indicator=null_indicator)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/accelerate/utils/operations.py", line 507, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast
    return func(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 454, in forward
    return self._forward(images = x, log_snr = log_snr, null_indicator=null_indicator)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 400, in _forward
    output, lowres_skips = stage(images, lowres_skips = lowres_skips, condition_context = condition_context, condition_embeds = condition_embeds, condition_masks = condition_masks, null_indicator=null_indicator)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 279, in forward
    x = block(x, condition_embeds, condition_masks, pos=pos)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 172, in forward
    return torch.utils.checkpoint.checkpoint(self._forward, x, condition_embeds, condition_masks, pos)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/_compile.py", line 24, in inner
    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py", line 328, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/_dynamo/external_utils.py", line 17, in inner
    return fn(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 451, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 230, in forward
    outputs = run_function(*args)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 178, in _forward
    x = x + self.dropout(self.block2(x))
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 97, in forward
    x = self.w2(x)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 2 has a total capacty of 31.74 GiB of which 33.38 MiB is free. Process 1182461 has 31.70 GiB memory in use. Of the allocated memory 30.96 GiB is allocated by PyTorch, and 5.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_t2i.py", line 362, in <module>
    app.run(main)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/absl/app.py", line 316, in run
    _run_main(main, args)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/absl/app.py", line 261, in _run_main
    sys.exit(main(argv))
  File "train_t2i.py", line 358, in main
    train(config)
  File "train_t2i.py", line 259, in train
    metrics = train_step(batch, ss_empty_context)
  File "train_t2i.py", line 174, in train_step
    loss, loss_dict = _flow_mathcing_model(_z, nnet, loss_coeffs=config.loss_coeffs, cond=_batch_con, con_mask=_batch_mask, batch_img_clip=_batch_img_ori, \
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/diffusion/flow_matching.py", line 339, in forward
    return self.p_losses_textVAE(
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/diffusion/flow_matching.py", line 424, in p_losses_textVAE
    prediction = nnet(x_noisy, log_snr = log_snr, null_indicator=null_indicator)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/accelerate/utils/operations.py", line 507, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast
    return func(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 454, in forward
    return self._forward(images = x, log_snr = log_snr, null_indicator=null_indicator)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 400, in _forward
    output, lowres_skips = stage(images, lowres_skips = lowres_skips, condition_context = condition_context, condition_embeds = condition_embeds, condition_masks = condition_masks, null_indicator=null_indicator)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 279, in forward
    x = block(x, condition_embeds, condition_masks, pos=pos)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 172, in forward
    return torch.utils.checkpoint.checkpoint(self._forward, x, condition_embeds, condition_masks, pos)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/_compile.py", line 24, in inner
    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py", line 328, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/_dynamo/external_utils.py", line 17, in inner
    return fn(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 451, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 230, in forward
    outputs = run_function(*args)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 178, in _forward
    x = x + self.dropout(self.block2(x))
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 96, in forward
    x = self.act(self.w0(x)) * self.w1(x)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 5.38 MiB is free. Process 1182460 has 31.73 GiB memory in use. Of the allocated memory 31.02 GiB is allocated by PyTorch, and 8.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_t2i.py", line 362, in <module>
    app.run(main)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/absl/app.py", line 316, in run
    _run_main(main, args)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/absl/app.py", line 261, in _run_main
    sys.exit(main(argv))
  File "train_t2i.py", line 358, in main
    train(config)
  File "train_t2i.py", line 259, in train
    metrics = train_step(batch, ss_empty_context)
  File "train_t2i.py", line 174, in train_step
    loss, loss_dict = _flow_mathcing_model(_z, nnet, loss_coeffs=config.loss_coeffs, cond=_batch_con, con_mask=_batch_mask, batch_img_clip=_batch_img_ori, \
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/diffusion/flow_matching.py", line 339, in forward
    return self.p_losses_textVAE(
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/diffusion/flow_matching.py", line 424, in p_losses_textVAE
    prediction = nnet(x_noisy, log_snr = log_snr, null_indicator=null_indicator)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/accelerate/utils/operations.py", line 507, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast
    return func(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 454, in forward
    return self._forward(images = x, log_snr = log_snr, null_indicator=null_indicator)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 400, in _forward
    output, lowres_skips = stage(images, lowres_skips = lowres_skips, condition_context = condition_context, condition_embeds = condition_embeds, condition_masks = condition_masks, null_indicator=null_indicator)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 279, in forward
    x = block(x, condition_embeds, condition_masks, pos=pos)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 172, in forward
    return torch.utils.checkpoint.checkpoint(self._forward, x, condition_embeds, condition_masks, pos)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/_compile.py", line 24, in inner
    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py", line 328, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/_dynamo/external_utils.py", line 17, in inner
    return fn(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 451, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 230, in forward
    outputs = run_function(*args)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 178, in _forward
    x = x + self.dropout(self.block2(x))
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 96, in forward
    x = self.act(self.w0(x)) * self.w1(x)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 13.38 MiB is free. Process 1182459 has 31.72 GiB memory in use. Of the allocated memory 30.99 GiB is allocated by PyTorch, and 9.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "train_t2i.py", line 362, in <module>
    app.run(main)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/absl/app.py", line 316, in run
    _run_main(main, args)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/absl/app.py", line 261, in _run_main
    sys.exit(main(argv))
  File "train_t2i.py", line 358, in main
    train(config)
  File "train_t2i.py", line 259, in train
    metrics = train_step(batch, ss_empty_context)
  File "train_t2i.py", line 174, in train_step
    loss, loss_dict = _flow_mathcing_model(_z, nnet, loss_coeffs=config.loss_coeffs, cond=_batch_con, con_mask=_batch_mask, batch_img_clip=_batch_img_ori, \
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/diffusion/flow_matching.py", line 339, in forward
    return self.p_losses_textVAE(
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/diffusion/flow_matching.py", line 424, in p_losses_textVAE
    prediction = nnet(x_noisy, log_snr = log_snr, null_indicator=null_indicator)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/accelerate/utils/operations.py", line 507, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/amp/autocast_mode.py", line 16, in decorate_autocast
    return func(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1519, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1355, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 454, in forward
    return self._forward(images = x, log_snr = log_snr, null_indicator=null_indicator)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 400, in _forward
    output, lowres_skips = stage(images, lowres_skips = lowres_skips, condition_context = condition_context, condition_embeds = condition_embeds, condition_masks = condition_masks, null_indicator=null_indicator)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 279, in forward
    x = block(x, condition_embeds, condition_masks, pos=pos)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 172, in forward
    return torch.utils.checkpoint.checkpoint(self._forward, x, condition_embeds, condition_masks, pos)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/_compile.py", line 24, in inner
    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py", line 328, in _fn
    return fn(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/_dynamo/external_utils.py", line 17, in inner
    return fn(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 451, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 230, in forward
    outputs = run_function(*args)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 178, in _forward
    x = x + self.dropout(self.block2(x))
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/storage/v-jinpewang/lab_folder/qisheng_azure/CrossFlow/libs/model/dimr_t2i.py", line 96, in forward
    x = self.act(self.w0(x)) * self.w1(x)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 3 has a total capacty of 31.74 GiB of which 9.38 MiB is free. Process 1182462 has 31.73 GiB memory in use. Of the allocated memory 30.99 GiB is allocated by PyTorch, and 9.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /storage/v-jinpewang/lab_folder/qisheng_data/workdir/t2i_training_demo/default/wandb/offline-run-20250412_161928-7256021883.54740-a4fd6b79-1f6c-477f-87b6-9e0c013335a9[0m
[1;34mwandb[0m: Find logs at: [1;35m../../qisheng_data/workdir/t2i_training_demo/default/wandb/offline-run-20250412_161928-7256021883.54740-a4fd6b79-1f6c-477f-87b6-9e0c013335a9/logs[0m
[2025-04-12 16:22:10,620] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 692745) of binary: /opt/conda/envs/crossflow/bin/python
Traceback (most recent call last):
  File "/opt/conda/envs/crossflow/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_t2i.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-04-12_16:22:10
  host      : 4ebf64dcfb814391b20452c5eacea59200001w.internal.cloudapp.net
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 692746)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-04-12_16:22:10
  host      : 4ebf64dcfb814391b20452c5eacea59200001w.internal.cloudapp.net
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 692747)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-04-12_16:22:10
  host      : 4ebf64dcfb814391b20452c5eacea59200001w.internal.cloudapp.net
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 692748)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-04-12_16:22:10
  host      : 4ebf64dcfb814391b20452c5eacea59200001w.internal.cloudapp.net
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 692745)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Traceback (most recent call last):
  File "/opt/conda/envs/crossflow/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/accelerate/commands/accelerate_cli.py", line 43, in main
    args.func(args)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/accelerate/commands/launch.py", line 831, in launch_command
    multi_gpu_launcher(args)
  File "/opt/conda/envs/crossflow/lib/python3.8/site-packages/accelerate/commands/launch.py", line 450, in multi_gpu_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['torchrun', '--nproc_per_node', '4', '--master_port', '16663', 'train_t2i.py', '--config=configs/t2i_training_demo.py']' returned non-zero exit status 1.
